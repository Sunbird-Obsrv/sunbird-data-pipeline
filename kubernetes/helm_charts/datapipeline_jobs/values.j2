namespace: {{ flink_namespace }}
imagepullsecrets: {{ imagepullsecrets }}
dockerhub: {{ dockerhub }}
repository: {{flink_repository|default('sunbird-datapipeline')}}
image_tag: {{ image_tag }}
azure_account: {{ azure_account }}
azure_secret: {{ azure_secret }}
serviceMonitor:
  enabled: {{ service_monitor_enabled | lower}}
replicaCount: {{taskmana_replicacount|default(1)}}

scale_properties:
  telemetry-extractor:
    enabled: {{ flink_job_names['telemetry-extractor'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['telemetry-extractor'].scale_target_value }}
    min_replica: {{ flink_job_names['telemetry-extractor'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['telemetry-extractor'].max_replica | default(2) }}
  pipeline-preprocessor:
    enabled: {{ flink_job_names['pipeline-preprocessor'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['pipeline-preprocessor'].scale_target_value }}
    min_replica: {{ flink_job_names['pipeline-preprocessor'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['pipeline-preprocessor'].max_replica | default(2) }}
  de-normalization-secondary:
    enabled: {{ flink_job_names['de-normalization-secondary'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['de-normalization-secondary'].scale_target_value }} 
    min_replica: {{ flink_job_names['de-normalization-secondary'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['de-normalization-secondary'].max_replica | default(2) }}
  de-normalization-primary:
    enabled: {{ flink_job_names['de-normalization-primary'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['de-normalization-primary'].scale_target_value }}
    min_replica: {{ flink_job_names['de-normalization-primary'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['de-normalization-primary'].max_replica | default(2) }}      
  druid-validator:
    enabled: {{ flink_job_names['druid-validator'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['druid-validator'].scale_target_value }}
    min_replica: {{ flink_job_names['druid-validator'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['druid-validator'].max_replica | default(2) }}
  assessment-aggregator:
    enabled: {{ flink_job_names['assessment-aggregator'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['assessment-aggregator'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['assessment-aggregator'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['assessment-aggregator'].max_replica | default(2) }}
  content-cache-updater:
    enabled: {{ flink_job_names['content-cache-updater'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['content-cache-updaterr'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['content-cache-updater'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['content-cache-updater'].max_replica | default(2) }}
  user-cache-updater-v2:
    enabled: {{ flink_job_names['user-cache-updater-v2'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['user-cache-updater-v2'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['user-cache-updater-v2'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['user-cache-updater-v2'].max_replica | default(2) }}
  summary-denormalization:
    enabled: {{ flink_job_names['summary-denormalization'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['summary-denormalization'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['summary-denormalization'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['summary-denormalization'].max_replica | default(2) }}
  device-profile-updater:
    enabled: {{ flink_job_names['device-profile-updater'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['device-profile-updater'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['device-profile-updater'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['device-profile-updater'].max_replica | default(2) }}
  ingest-router:
    enabled: {{ flink_job_names['ingest-router'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['ingest-router'].scale_target_value }}
    min_replica: {{ flink_job_names['ingest-router'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['ingest-router'].max_replica | default(2) }}
  error-denormalization:
    enabled: {{ flink_job_names['error-denormalization'].scale_enabled | lower}}
    scale_target_value: {{ flink_job_names['error-denormalization'].scale_target_value | default(0) }}
    min_replica: {{ flink_job_names['error-denormalization'].min_replica | default(1) }}
    max_replica: {{ flink_job_names['error-denormalization'].max_replica | default(2) }}

jobmanager:
  rpc_port: {{ jobmanager_rpc_port }}
  blob_port: {{ jobmanager_blob_port }}
  query_port: {{ jobmanager_query_port }}
  ui_port: {{ jobmanager_ui_port }}
  prom_port: {{ jobmanager_prom_port }}
  heap_memory: {{ jobmanager_heap_memory }}

service: {{ jobmanager_ui_service|to_json }}

rest_port: {{ jobmanager_ui_rest_port }}
resttcp_port: {{ jobmanager_ui_tcp_port }}

taskmanager:
  prom_port: {{ taskmanager_prom_port }}
  rpc_port: {{ taskmanager_rpc_port }}
  heap_memory: {{ taskmanager_heap_memory }}
  replicas: {{taskmanager_replicacount|default(1)}}
  cpu_requests: {{ cpu_requests|default(1)}}

job_classname: {{ job_classname }}
{{ taskmanager_liveness | to_nice_yaml }}

log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = {{ flink_jobs_console_log_level | default(INFO) }}
  rootLogger.appenderRef.console.ref = ConsoleAppender

  # Uncomment this if you want to _only_ change Flink's logging
  #logger.flink.name = org.apache.flink
  #logger.flink.level = {{ flink_jobs_console_log_level | default(INFO) }}

  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = {{ flink_libraries_log_level | default(INFO) }}
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = {{ flink_libraries_log_level | default(INFO) }}
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = {{ flink_libraries_log_level | default(INFO) }}
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = {{ flink_libraries_log_level | default(INFO) }}

  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF

base_config: |
  kafka {
      broker-servers = "{{ kafka_brokers }}"
      producer.broker-servers = "{{ kafka_brokers }}"
      consumer.broker-servers = "{{ kafka_brokers }}"
      zookeeper = "{{ zookeepers }}"
      producer {
        max-request-size = {{ producer_max_request_size }}
        batch.size = {{ producer_batch_size }}
        linger.ms = {{ producer_linger_ms }}
      }
    }
    job {
      env = "{{ env_name }}"
      enable.distributed.checkpointing = true
      statebackend {
        blob {
          storage {
            account = "{{ azure_account }}.blob.core.windows.net"
            container = "{{ flink_container_name }}"
            checkpointing.dir = "checkpoint"
          }
        }
        base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
      }
    }
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = {{ checkpoint_compression_enabled|lower }}
      checkpointing.interval = {{ checkpoint_interval }}
      checkpointing.pause.between.seconds = {{ checkpoint_pause_between_seconds }}
      restart-strategy.attempts = {{ restart_attempts }}
      restart-strategy.delay = {{ restart_delay }} # in milli-seconds
    }
    redisdb.connection.timeout = {{ redis_timeout }}
    redis {
      host = {{ redis_host }}
      port = 6379
    }
    redis-meta {
{% if metadata2_redis_host is defined %}
      host = {{ metadata2_redis_host }}
{% else %}
      host = {{ redis_host }}
{% endif %}
      port = 6379
    }
    postgres {
      host = {{ postgres.db_url }}
      port = {{ postgres.db_port }}
      maxConnections = {{ postgres_max_connections }}
      user = "{{ postgres.db_username }}"
      password = "{{ postgres.db_password }}"
    }
    lms-cassandra {
      host = "{{ core_cassandra_host }}"
      port = "9042"
    }

ingest-router:
  ingest-router: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      producer.broker-servers = "{{ kafka_brokers }}"
      consumer.broker-servers = "{{ ingestion_kafka_brokers }}"
      ingest {
       input.topic = {{ env_name }}.telemetry.ingestion
       output.success.topic = {{ env_name }}.telemetry.ingest
      }
      raw {
       input.topic = "{{ env_name }}.telemetry.raw"
       output.success.topic = "{{ env_name }}.telemetry.raw"
      }
      groupId = {{ env_name }}-ingest-router-group
      producer {
        max-request-size = {{ extractor_max_request_size }}
      }
    }
    task {
      consumer.parallelism = {{ ingest_router_consumer_parallelism }}
      downstream.operators.parallelism = {{ ingest_router_operators_parallelism }}
      raw {
       consumer.parallelism = {{ raw_router_consumer_parallelism }}
       downstream.operators.parallelism = {{ raw_router_downstream_parallelism }}
      }
    }
  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['ingest-router'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['ingest-router'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['ingest-router'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['ingest-router'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m


telemetry-extractor:
  telemetry-extractor: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.telemetry.ingest
      output.success.topic = {{ env_name }}.telemetry.raw
      output.log.route.topic = {{ env_name }}.druid.events.log
      output.duplicate.topic = {{ env_name }}.telemetry.extractor.duplicate
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.batch.failed.topic = {{ env_name }}.telemetry.extractor.failed
      output.assess.raw.topic = {{ env_name }}.telemetry.assess.raw
      event.max.size = "{{ extractor_event_max_size }}" # Max is only 1MB
      groupId = {{ env_name }}-telemetry-extractor-group
      producer {
        max-request-size = {{ extractor_max_request_size }}
      }
    }
    task {
      consumer.parallelism = {{ extractor_consumer_parallelism }}
      downstream.operators.parallelism = {{ extractor_operators_parallelism }}
    }
    redis {
      database {
        duplicationstore.id = 1
        key.expiry.seconds = {{ telemetry_extractor_key_expiry_seconds }} 
        contentstore.id = 5
      }
    }
    redis-meta {
        database {
          contentstore.id = 5
        }
        host = {{ redis_meta_content_host }}
        port = {{ redis_meta_content_port }}
      }
    redact.events.list = ["ASSESS","RESPONSE"]

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['telemetry-extractor'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['telemetry-extractor'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['telemetry-extractor'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['telemetry-extractor'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m

pipeline-preprocessor:
  pipeline-preprocessor: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.telemetry.raw
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.primary.route.topic = {{ env_name }}.telemetry.unique
      output.log.route.topic = {{ env_name }}.druid.events.log
      output.error.route.topic = {{ env_name }}.telemetry.error
      output.audit.route.topic = {{ env_name }}.telemetry.audit
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      output.denorm.secondary.route.topic = {{ env_name }}.telemetry.unique.secondary
      output.denorm.primary.route.topic = {{ env_name }}.telemetry.unique.primary
      groupId = {{ env_name }}-pipeline-preprocessor-group
    }
    task {
      consumer.parallelism = {{ pipeline_preprocessor_consumer_parallelism }}
      downstream.operators.parallelism = {{ pipeline_preprocessor_operators_parallelism }}
    }
    telemetry.schema.path="schemas/telemetry/3.0"
    default.channel="b00bc992ef25f1a9a8d63291e20efc8d"
    dedup.producer.included.ids = ["{{ portal_id }}", "{{ desktop_id }}"]
    secondary.events = ["INTERACT", "IMPRESSION", "SHARE_ITEM"]
    redis {
      database {
        duplicationstore.id = 7
        key.expiry.seconds = {{ pipeline_preprocessor_key_expiry_seconds }}
      }
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['pipeline-preprocessor'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['pipeline-preprocessor'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['pipeline-preprocessor'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['pipeline-preprocessor'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m

de-normalization-secondary:
  de-normalization-secondary: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.telemetry.topic = {{ env_name }}.telemetry.unique.secondary
      input.summary.topic = {{ env_name }}.telemetry.derived
      telemetry.denorm.output.topic = {{ env_name }}.telemetry.denorm
      summary.denorm.output.topic = {{ env_name }}.druid.events.summary
      summary.unique.events.topic = {{ env_name }}.telemetry.derived.unique
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      groupId = {{ env_name }}-telemetry-denorm-secondary-group
    }
    skip.events = ["INTERRUPT"]
    task {
      window.count = {{ denorm_secondary_window_count }}
      window.shards = {{ denorm_secondary_window_shards }}
      consumer.parallelism = {{ denorm_secondary_consumer_parallelism }}
      telemetry.downstream.operators.parallelism = {{ telemetry_denorm_secondary_operators_parallelism }}
      summary.downstream.operators.parallelism = {{ summary_denorm_operators_parallelism  }}
    }
    redis {
      database {
        duplicationstore.id = 9
        key.expiry.seconds = {{ de_normalization_duplicationstore_key_expiry_seconds }} 
      }
    }
    # redis-metadata
    redis-meta {
      database {
        devicestore.id = 2
        userstore.id = 12
        contentstore.id = 5
        dialcodestore.id = 6
        key.expiry.seconds = {{ de_normalization_key_expiry_seconds }}
      }
      content.host = {{ redis_meta_content_host }}
      device.host = {{ redis_meta_device_host }}
      user.host = {{ redis_meta_user_host }}
      dialcode.host = {{ redis_meta_dialcode_host }}
      content.port = {{ redis_meta_content_port }}
      device.port = {{ redis_meta_device_port }}
      user.port = {{ redis_meta_user_port }}
      dialcode.port = {{ redis_meta_dialcode_port }}
    }
    # config version
    user_denorm_version = v2

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['de-normalization-secondary'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['de-normalization-secondary'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['de-normalization-secondary'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['de-normalization-secondary'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m

de-normalization-primary:
  de-normalization-primary: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.telemetry.topic = {{ env_name }}.telemetry.unique.primary
      input.summary.topic = {{ env_name }}.telemetry.derived
      telemetry.denorm.output.topic = {{ env_name }}.telemetry.denorm
      summary.denorm.output.topic = {{ env_name }}.druid.events.summary
      summary.unique.events.topic = {{ env_name }}.telemetry.derived.unique
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      groupId = {{ env_name }}-telemetry-denorm-primary-group
    }
    skip.events = ["INTERRUPT"]
    task {
      window.count = {{ denorm_primary_window_count }}
      window.shards = {{ denorm_primary_window_shards }}
      consumer.parallelism = {{ denorm_primary_consumer_parallelism }}
      telemetry.downstream.operators.parallelism = {{ telemetry_denorm_primary_operators_parallelism }}
      summary.downstream.operators.parallelism = {{ summary_denorm_operators_parallelism  }}
    }
    redis {
      database {
        duplicationstore.id = 9
        key.expiry.seconds = {{ de_normalization_duplicationstore_key_expiry_seconds }} 
      }
    }
    # redis-metadata
    redis-meta {
      database {
        devicestore.id = 2
        userstore.id = 12
        contentstore.id = 5
        dialcodestore.id = 6
        key.expiry.seconds = {{ de_normalization_key_expiry_seconds }}
      }
      content.host = {{ redis_meta_content_host }}
      device.host = {{ redis_meta_device_host }}
      user.host = {{ redis_meta_user_host }}
      dialcode.host = {{ redis_meta_dialcode_host }}
      content.port = {{ redis_meta_content_port }}
      device.port = {{ redis_meta_device_port }}
      user.port = {{ redis_meta_user_port }}
      dialcode.port = {{ redis_meta_dialcode_port }}
    }
    # config version
    user_denorm_version = v2

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['de-normalization-primary'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['de-normalization-primary'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['de-normalization-primary'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['de-normalization-primary'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m   

summary-denormalization:
  summary-denormalization: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.telemetry.topic = {{ env_name }}.telemetry.unique
      input.summary.topic = {{ env_name }}.telemetry.derived
      telemetry.denorm.output.topic = {{ env_name }}.telemetry.denorm
      summary.denorm.output.topic = {{ env_name }}.druid.events.summary
      summary.unique.events.topic = {{ env_name }}.telemetry.derived.unique
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      groupId = {{ env_name }}-summmary-denorm-group
    }
    skip.events = ["INTERRUPT"]
    task {
      window.count = {{ denorm_summary_window_count }}
      window.shards = {{ denorm_summary_window_shards }}
      consumer.parallelism = {{ summary_denorm_consumer_parallelism }}
      telemetry.downstream.operators.parallelism = {{ telemetry_denorm_operators_parallelism }}
      summary.downstream.operators.parallelism = {{ summary_denorm_operators_parallelism  }}
    }
    redis {
      database {
        duplicationstore.id = 9
        key.expiry.seconds = {{ summary_denorm_duplication_key_expiry_seconds }}
      }
    }
    # redis-metadata
    redis-meta {
      database {
        devicestore.id = 2
        userstore.id = 12
        contentstore.id = 5
        dialcodestore.id = 6
        key.expiry.seconds = {{ summary_denorm_key_expiry_seconds }}
      }
      content.host = {{ redis_meta_content_host }}
      device.host = {{ redis_meta_device_host }}
      user.host = {{ redis_meta_user_host }}
      dialcode.host = {{ redis_meta_dialcode_host }}
      content.port = {{ redis_meta_content_port }}
      device.port = {{ redis_meta_device_port }}
      user.port = {{ redis_meta_user_port }}
      dialcode.port = {{ redis_meta_dialcode_port }}
    }
    # config version
    user_denorm_version = v2

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['summary-denormalization'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['summary-denormalization'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['summary-denormalization'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

druid-validator:
  druid-validator: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.telemetry.denorm
      output.telemetry.route.topic = {{ env_name }}.druid.events.telemetry
      output.summary.route.topic = {{ env_name }}.druid.events.summary
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      groupId = {{ env_name }}-druid-validator-group
    }
    task {
      consumer.parallelism = {{ druid_validator_consumer_parallelism }}
      downstream.operators.parallelism = {{ druid_validator_operators_parallelism }}
      druid.validation.enabled = {{ druid_validation_enabled|lower }}
      druid.deduplication.enabled = {{ druid_deduplication_enabled|lower }}
    }
    schema {
      path {
        telemetry = "schemas/telemetry"
        summary = "schemas/summary"
      }
      file {
        default = envelope.json
        summary = me_workflow_summary.json
        search = search.json
      }
    }
    redis {
      database {
        duplicationstore.id = 8
        key.expiry.seconds = {{ druid_validator_key_expiry_seconds }}
      }
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['druid-validator'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['druid-validator'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['druid-validator'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: {{ flink_job_names['druid-validator'].taskmanager_process_memory }}
    jobmanager.memory.process.size: 1600m

device-profile-updater:
  device-profile-updater: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      producer.broker-servers = "{{ ingestion_kafka_brokers }}"
      consumer.broker-servers = "{{ ingestion_kafka_brokers }}"
      zookeeper = "{{ ingestion_zookeepers }}"
      input.topic = {{ env_name }}.events.deviceprofile
      groupId = {{ env_name }}-device-profile-updater-group
    }
    task {
      deviceprofile {
        parallelism = {{ deviceprofile_parallelism }}
      }
    }
    # redis-metadata
    redis-meta {
      database {
        devicestore.id = 2
      }
      host = {{ redis_meta_device_host }}
      port = {{ redis_meta_device_port }}
    }
    postgres {
      database = "{{ postgres.db_name }}",
      table = "{{ device_profile_table }}"
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['device-profile-updater'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['device-profile-updater'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['device-profile-updater'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

content-cache-updater:
  content-cache-updater: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      producer.broker-servers = "{{ ingestion_kafka_brokers }}"
      consumer.broker-servers = "{{ ingestion_kafka_brokers }}"
      zookeeper = "{{ ingestion_zookeepers }}"
      input.topic = {{ env_name }}.learning.graph.events
      groupId = {{ env_name }}-content-cache-updater-group
    }
    # redis-metadata
    redis-meta {
      database {
        contentstore.id = 5
        dialcodestore.id = 6
      }
      content.host = {{ redis_meta_content_host }}
      dialcode.host = {{ redis_meta_dialcode_host }}
      content.port = {{ redis_meta_content_port }}
      dialcode.port = {{ redis_meta_dialcode_port }}
    }
    dialcode {
      api {
        url = "{{ dialcode_api_url }}"
        token = "{{ sunbird_api_auth_token }} "
      }
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['content-cache-updater'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['content-cache-updater'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['content-cache-updater'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

user-cache-updater-v2:
  user-cache-updater-v2: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.telemetry.audit
      groupId = {{ env_name }}-user-cache-updater-consumer-group
    }
    task {
      usercache.updater.parallelism = {{ usercache_updater_parallelism }}
    }
    # redis-metadata
    redis-meta {
      database {
        userstore.id = 12
      }
      host = {{ redis_meta_user_host }}
      port = {{ redis_meta_user_port }}
    }
    user-read {
     api {
       url = "{{ user_read_api_url  }}"
     }
    }
    regd.user.producer.pid = "learner-service"
    user.self.signin.types = ["google","self"]
    user.validated.types = ["sso"]
    user.self.signin.key = "Self-Signed-In"
    user.valid.key = "Validated"
    user.read.url.fields = "locations,organisations"
    user.read.api.error = ["CLIENT_ERROR"]

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['user-cache-updater-v2'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['user-cache-updater-v2'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['user-cache-updater-v2'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

assessment-aggregator: 
  assessment-aggregator: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      producer.broker-servers = "{{ ingestion_kafka_brokers }}"
      consumer.broker-servers = "{{ ingestion_kafka_brokers }}"
      zookeeper = "{{ ingestion_zookeepers }}"
      input.topic = {{ env_name }}.telemetry.assess
      failed.topic= {{ env_name }}.telemetry.assess.failed
      groupId = {{ env_name }}-assessment-aggregator-group
      output.certissue.topic = {{ env_name }}.issue.certificate.request
    }
    task {
      consumer.parallelism = {{ assessaggregator_consumer_parallelism }}
      downstream.parallelism = {{ assessaggregator_downstream_parallelism }}
      assessaggregator {
        parallelism = {{ assessaggregator_parallelism }}
      }
      scoreaggregator.parallelism = {{ assessaggregator_scoreaggregator_parallelism }}
    }
    lms-cassandra {
      keyspace = "{{ middleware_cassandra_courses_keyspace }}"
      table = "{{ middleware_cassandra_assessment_aggregator_table }}"
      questionudttype= "{{ middleware_cassandra_assessment_question_type }}"
      enrolmentstable = "{{ middleware_cassandra_user_enrolments_table }}"
      activitytable = "{{ middleware_cassandra_user_activity_agg_table }}"
    }
    redis {
      database {
        relationCache.id = 10
      }
    }
  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['assessment-aggregator'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['assessment-aggregator'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['assessment-aggregator'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

error-denormalization:
  error-denormalization: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.telemetry.topic = {{ env_name }}.telemetry.error
      input.summary.topic = {{ env_name }}.telemetry.derived
      telemetry.denorm.output.topic = {{ env_name }}.druid.events.error
      summary.denorm.output.topic = {{ env_name }}.druid.events.summary
      summary.unique.events.topic = {{ env_name }}.telemetry.derived.unique
      output.failed.topic = {{ env_name }}.telemetry.failed
      output.duplicate.topic = {{ env_name }}.telemetry.duplicate
      groupId = {{ env_name }}-error-denorm-group
    }
    skip.events = ["INTERRUPT"]
    task {
      window.count = {{ denorm_primary_window_count }}
      window.shards = {{ denorm_primary_window_shards }}
      consumer.parallelism = {{ error_denorm_consumer_parallelism }}
      telemetry.downstream.operators.parallelism = {{ error_denorm_operators_parallelism }}
      summary.downstream.operators.parallelism = {{ summary_denorm_operators_parallelism  }}
    }
    redis {
      database {
        duplicationstore.id = 9
        key.expiry.seconds = {{ de_normalization_duplicationstore_key_expiry_seconds }} 
      }
    }
    # redis-metadata
    redis-meta {
      database {
        devicestore.id = 2
        userstore.id = 12
        contentstore.id = 5
        dialcodestore.id = 6
        key.expiry.seconds = {{ de_normalization_key_expiry_seconds }}
      }
      content.host = {{ redis_meta_content_host }}
      device.host = {{ redis_meta_device_host }}
      user.host = {{ redis_meta_user_host }}
      dialcode.host = {{ redis_meta_dialcode_host }}
      content.port = {{ redis_meta_content_port }}
      device.port = {{ redis_meta_device_port }}
      user.port = {{ redis_meta_user_port }}
      dialcode.port = {{ redis_meta_dialcode_port }}
    }
    # config version
    user_denorm_version = v2

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['error-denormalization'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['error-denormalization'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['error-denormalization'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
