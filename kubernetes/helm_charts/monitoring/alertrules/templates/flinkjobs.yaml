---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: alert-rules
    app: {{ .Values.prometheus_rule_selector_app }}
    release: {{ .Values.prometheus_rule_selector_release }}
  name: {{ .Values.fullnameOverride }}-flinkjobs-rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: alertrules.flinkjobs
    rules:
    - alert: telemetry-extractor lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "telemetry-extractor-taskmanager-prometheus" }) >  {{ .Values.telemetry_extractor_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "telemetry-extractor-taskmanager-prometheus" }) > {{ .Values.telemetry_extractor_threshold_critical }} 
        summary: telemetry-extractor lag is critical

    - alert: pipeline-preprocessor lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "pipeline-preprocessor-taskmanager-prometheus" }) > {{ .Values.pipeline_preprocessor_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "pipeline-preprocessor-taskmanager-prometheus" }) > {{ .Values.pipeline_preprocessor_threshold_critical }} 
        summary: pipeline-preprocessor lag is critical
   
    - alert:  de-normalization lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "de-normalization-taskmanager-prometheus" }) > {{ .Values.de_normalization_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "de-normalization-taskmanager-prometheus" }) > {{ .Values.de_normalization_threshold_critical }}
        summary: de-normalization lag is critical

    - alert:  druid-validator lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "druid-validator-taskmanager-prometheus" }) > {{ .Values.druid_validator_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "drud-validator-taskmanager-prometheus" }) > {{ .Values.druid_validator_threshold_critical }}
        summary: druid-validator lag is critical

    - alert: assessment-aggregator lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "assessment-aggregator-taskmanager-prometheus" }) > {{ .Values.assessment_aggregator_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "assessment-aggregator-taskmanager-prometheus" }) > {{ .Values.assessment_aggregator_threshold_critical }}
        summary: assessment-aggregator lag is critical

    - alert: content-cache-updater lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "content-cache-updater-taskmanager-prometheus" }) > {{ .Values.content_cache_updater_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "content-cache-updater-taskmanager-prometheus" }) > {{ .Values.content_cache_updater_threshold_critical }}
        summary: content-cache-updater lag is critical

    - alert: user-cache-updater lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "user-cache-updater-taskmanager-prometheus" }) > {{ .Values.user_cache_updater_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "user-cache-updater-taskmanager-prometheus" }) > {{ .Values.user_cache_updater_threshold_critical }}
        summary: user-cache-updater lag is critical

    - alert: summary-denormalization lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "summary-denormalization-taskmanager-prometheus" }) > {{ .Values.summary_denormalization_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "summary-denormalization-taskmanager-prometheus" }) > {{ .Values.summary_denormalization_threshold_critical }}
        summary: summary-denormalization lag is critical

    - alert: device-profile-updater lag critical
      expr: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "device-profile-updater-taskmanager-prometheus" }) > {{ .Values.device_profile_updater_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_taskmanager_job_task_operator_KafkaConsumer_records_lag_max { job = "device-profile-updater-taskmanager-prometheus" }) > {{ .Values._device_profile_updater_threshold_critical }}
        summary: device-profile-updater lag is critical

#### Checkpoint failure alert rules
    
    - alert: telemetry-extractor checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "telemetry-extractor-jobmanager" }) > {{ .Values.telemetry_extractor_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "telemetry-extractor-jobmanager" }) > {{ .Values.telemetry_extractor_checkpointfailure_threshold_critical }} 
        summary: telemetry-extractor checkpoint failure critical

    - alert: pipeline-preprocessor checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "pipeline-preprocessor-jobmanager" }) > {{ .Values.pipeline_preprocessor_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "pipeline-preprocessor-jobmanager" }) > {{ .Values.pipeline_preprocessor_checkpointfailure_threshold_critical }} 
        summary: pipeline-preprocessor checkpoint failure critical

    - alert: de-normalization checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "de-normalization-jobmanager" }) > {{ .Values.de_normalization_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "de-normalization-jobmanager" }) > {{ .Values.de_normalization_checkpointfailure_threshold_critical }} 
        summary: de-normalization checkpoint failure critical

    - alert: druid-validator checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "druid-validator-jobmanager" }) > {{ .Values.druid_validator_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "druid-validator-jobmanager" }) > {{ .Values.druid_validator_checkpointfailure_threshold_critical }} 
        summary: druid-validator checkpoint failure critical

    - alert: assessment-aggregator checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "assessment-aggregator-jobmanager" }) > {{ .Values.assessment_aggregator_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "assessment-aggregator-jobmanager" }) > {{ .Values.assessment_aggregator_checkpointfailure_threshold_critical }} 
        summary: assessment-aggregator checkpoint failure critical

    - alert: content-cache-updater checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "content-cache-updater-jobmanager" }) > {{ .Values.content_cache_updater_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "content-cache-updater-jobmanager" }) > {{ .Values.content_cache_updater_checkpointfailure_threshold_critical }} 
        summary: content-cache-updater checkpoint failure critical

    - alert: user-cache-updater checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "user-cache-updater-jobmanager" }) > {{ .Values.user_cache_updater_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "user-cache-updater-jobmanager" }) > {{ .Values.user_cache_updater_checkpointfailure_threshold_critical }} 
        summary: user-cache-updater checkpoint failure critical

    - alert: summary-denormalization checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "summary-denormalization-jobmanager" }) > {{ .Values.summary_denormalization_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "summary-denormalization-jobmanager" }) > {{ .Values.summary_denormalization_checkpointfailure_threshold_critical }} 
        summary: summary-denormalization checkpoint failure critical

    - alert: device-profile-updater checkpoint failure critical
      expr: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "device-profile-updater-jobmanager" }) > {{ .Values.device_profile_updater_checkpointfailure_threshold_critical }}
      for: 5m
      labels:
        severity: critical
      annotations:
        message: sum(flink_jobmanager_job_numberOfFailedCheckpoints { job = "device_profile_updater-jobmanager" }) > {{ .Values.device_profile_updater_checkpointfailure_threshold_critical }} 
        summary: device-profile-updater checkpoint failure critical

### telemetry extractor failed event percentage alert rule

    - alert: telemetry-extractor failed events percentage fatal
      expr: sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m])) / (sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_success_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_duplicate_event_count[5m]))) * 100 > {{ .Values.telemetry_extractor_failed_events_percentage_threshold_fatal }}
      for: 5m
      labels:
        severity: fatal
      annotations:
        message: sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m])) / (sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_success_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_duplicate_event_count[5m]))) * 100 > {{ .Values.telemetry_extractor_failed_events_percentage_threshold_fatal }} 
        summary: telemetry-extractor failed events percentage fatal

    - alert: telemetry-extractor failed events percentage critical
      expr: sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m])) / (sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_success_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_duplicate_event_count[5m]))) * 100 > {{ .Values.telemetry_extractor_failed_events_percentage_threshold_critical }}
      for: 5m
      labels:
        severity: fatal
      annotations:
        message: sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m])) / (sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_failed_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_success_batch_count[5m]))  + sum(sum_over_time(flink_taskmanager_job_task_operator_TelemetryExtractorJob_duplicate_event_count[5m]))) * 100 > {{ .Values.telemetry_extractor_failed_events_percentage_threshold_critical }}
        summary: telemetry-extractor failed events percentage critical
