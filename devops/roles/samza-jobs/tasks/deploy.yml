---
- name: Create Directory for Jobs
  file: path={{item}} owner=hduser group=hadoop recurse=yes state=directory
  with_items:
    - "{{samza_jobs_dir}}"
    - "{{samza_jobs_dir}}/extract"

- name: Copy script to get all running jobs
  copy: src=get_all_running_app_name.sh dest=/usr/local/hadoop/bin owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to get all job names
  copy: src=get_all_job_name.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to get updated job names from extracted tar
  copy: src=update_new_job_name.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to start jobs based on the status
  copy: src=start_jobs.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to remove old job tar
  copy: src=remove_old_tar.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to kill jobs based on the status
  copy: src=kill_jobs.sh dest=/usr/local/hadoop/bin owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Remove file of job status
  file: path="{{job_status_file}}" state=absent

- name: Get job names from folder
  command: bash -lc "./get_all_job_name.sh {{job_status_file}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract"

- name: Ensure yarn resource manager is running
  command: bash -lc "(ps aux | grep yarn-hduser-resourcemanager | grep -v grep) || /usr/local/hadoop/sbin/yarn-daemon.sh --config /usr/local/hadoop-{{hadoop_version}}/conf/ start resourcemanager"
  become: yes
  become_user: hduser

- name: Update status of running job in file
  command: bash -lc "./get_all_running_app_name.sh {{job_status_file}}"
  args:
    chdir: /usr/local/hadoop/bin

- name: copy new jobs tar ball
  copy: src={{ item }} dest={{samza_jobs_dir}}/ force=no owner=hduser group=hadoop
  with_fileglob:
    - ./jobs/*
  register: new_jobs

- name: Create Directory to extract new jobs
  file: path={{samza_jobs_dir}}/extract/{{item.item | basename }} owner=hduser group=hadoop recurse=yes state=directory
  register: extract_dir
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: extract new jobs
  command: tar -xvf "{{samza_jobs_dir}}/{{item.item | basename}}" -C "{{samza_jobs_dir}}/extract/{{item.item | basename }}"
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: Create Directory to extract new jobs
  file: path={{samza_jobs_dir}}/extract/ owner=hduser group=hadoop recurse=yes

- name: Get all new job configs
  shell: "ls -d -1 {{item.path}}/config/*.properties"
  register: config_files
  when: "{{item|changed}}"
  with_items: "{{ (extract_dir|default({})).results|default([]) }}"


- name: update environment specific details in new job configs
  replace: dest="{{item[1].stdout}}" regexp="{{item[0].key}}" replace="{{item[0].value}}"
  when: "{{item[1]|changed}}"
  with_nested:
    - [{key: "__yarn_host__", value: "{{__yarn_host__}}"}, {key: "__yarn_port__", value: "{{__yarn_port__}}"}, {key: "__env__", value: "{{env}}" }, {key: "__env_name__", value: "{{env_name}}" }, {key: "__zookeepers__", value: "{{zookeepers}}"}, {key: "__kafka_brokers__", value: "{{kafka_brokers}}"}, {key: "__delayInMilliSeconds__", value: "{{delayInMilliSeconds}}" }, {key: "__retryTimeInMilliSeconds__", value: "{{retryTimeInMilliSeconds}}" }, {key: "__bypass_reverse_search__", value: "{{bypass_reverse_search}}" }, {key: "__retryBackoffBaseInSeconds__", value: "{{retry_backoff_base_in_seconds}}" }, {key: "__retryLimit__", value: "{{retry_limit}}" }, {key: "__retryLimitEnable__", value: "{{retry_limit_enable}}" },  {key: "__google_api_key__", value: "{{google_api_key}}" }, {key: "__searchServiceEndpoint__", value: "{{search_service_endpoint}}" }, {key: "__objectDenormalizationAdditionalConfig__", value: "{{object_denormalization_additional_config}}" }, {key: "__environment_id__", value: "{{environment_id}}"}, {key: "__google_vision_tagging__", value: "{{google_vision_tagging}}"},{key: "__esRouterAdditionalConfig__", value: "{{es_router_additional_config}}"},{key: "__esRouterSecondaryAdditionalConfig__", value: "{{es_router_additional_secondary_config}}"},{key: "__es_port__", value: "{{es_port}}"},{key: "__cassandra_host__", value: "{{cassandra_host}}"},{key: "__cassandra_port__", value: "{{cassandra_port}}"}, {key: "__content_to_vec_url__", value: "{{content_to_vec_url}}"},{key: "__max_iteration_count_for_samza_job__", value: "{{max_iteration_count_for_samza_job}}"},{key: "__telemetry_extractor_yarn_container_count__", value: "{{telemetry_extractor_yarn_container_count}}"},{key: "__telemetry_validator_yarn_container_count__", value: "{{telemetry_validator_yarn_container_count}}"},{key: "__telemetry_de_duplication_yarn_container_count__", value: "{{telemetry_de_duplication_yarn_container_count}}"},{key: "__telemetry_router_yarn_container_count__", value: "{{telemetry_router_yarn_container_count}}"},{key: "__telemetry_reverse_search_yarn_container_count__", value: "{{telemetry_reverse_search_yarn_container_count}}"},{key: "__telemetry_object_de_normalization_yarn_container_count__", value: "{{telemetry_object_de_normalization_yarn_container_count}}"}, {key: "__telemetry_es_indexer_yarn_container_count__", value: "{{telemetry_es_indexer_yarn_container_count}}"}, {key: "__events_router_yarn_container_count__", value: "{{events_router_yarn_container_count}}"}, {key: "__telemetry_extractor_consumer_fetch_max_bytes__", value: "{{telemetry_extractor_consumer_fetch_max_bytes}}"}, {key: "__telemetry_extractor_container_memory_mb__", value: "{{telemetry_extractor_container_memory_mb}}"}, {key: "__ingestion_zookeepers__", value: "{{ingestion_zookeepers}}"},{key: "__tr_secondary_route_events__", value: "{{tr_secondary_route_events}}"},{key: "__telemetry_schema_path__", value: "{{telemetry_schema_path}}"},{key: "__yarn_es_hosts__", value: "{{yarn_es_hosts}}"},{key: "__default_channel__", value: "{{default_channel}}"},{key: "__telemetry_location_updater_yarn_container_count__", value: "{{telemetry_location_updater_yarn_container_count}}"},{key: "__channelSearchServiceEndpoint__", value: "{{channelSearchServiceEndpoint}}"},{key: "__locationSearchServiceEndpoint__", value: "{{locationSearchServiceEndpoint}}"},{key: "__searchServiceAuthorizationToken__", value: "{{searchServiceAuthorizationToken}}"},{key: "__redis_host__", value: "{{redis_host}}"}, {key: "__redis_port__", value: "{{redis_port}}"},{key: "__location_db_redis_key_expiry_seconds__", value: "{{location_db_redis_key_expiry_seconds}}"}, {key: "__middleware_cassandra_host__", value: "{{middleware_cassandra_host}}"},{key: "__middleware_cassandra_port__", value: "{{middleware_cassandra_port}}"},{key: "__middleware_cassandra_keyspace__", value: "{{middleware_cassandra_keyspace}}"}, {key: "__middleware_cassandra_user_table__", value: "{{middleware_cassandra_user_table}}"}, {key: "__middleware_cassandra_location_table__", value: "{{middleware_cassandra_location_table}}"}]
    - "{{ (config_files|default({})).results|default([]) }}"

- name: Create directory for additional config
  file: path={{object_denormalization_additional_config_dir}} owner=hduser group=hadoop recurse=yes state=directory

- name: Update status of new jobs in file
  command: bash -lc "./update_new_job_name.sh {{job_status_file}} {{samza_jobs_dir}}/extract/{{item.item | basename}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: Kill jobs
  command: bash -lc "./kill_jobs.sh {{job_status_file}}"
  args:
    chdir: /usr/local/hadoop/bin

- name: Start jobs
  command: bash -lc "./start_jobs.sh {{job_status_file}} {{samza_jobs_dir}}/extract"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"
  become_user: hduser

- name: Remove all old tar
  command: bash -lc "./remove_old_tar.sh {{job_status_file}} {{samza_jobs_dir}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"

- file: path={{samza_jobs_dir}} owner=hduser group=hadoop state=directory recurse=yes
