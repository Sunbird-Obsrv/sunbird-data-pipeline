## Provision Spark ##

- name: Download Apache Spark {{ spark_version }} hadoop 2.7
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ spark_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Unarchive Spark {{ spark_version }} hadoop 2.7
  become: yes
  become_user: "{{ analytics_user }}"
  unarchive: src={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz dest={{ analytics.home }}/ copy=no owner={{ analytics_user }} group={{ analytics_group }} creates={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7

- name: Download Scala 2.12.10
  become: yes
  become_user: "{{ analytics_user }}"
  shell: wget -O {{ analytics.soft_path }}/scala-{{ scala_version }}.tgz https://downloads.lightbend.com/scala/{{ scala_version }}/scala-{{ scala_version }}.tgz

- name: Unarchive Scala 2.12.10
  become: yes
  become_user: "{{ analytics_user }}"
  unarchive: src={{ analytics.soft_path }}/scala-{{ scala_version }}.tgz dest={{ analytics.soft_path }}/ copy=no owner={{ analytics_user }} group={{ analytics_group }} creates={{ analytics.soft_path }}/scala-{{ scala_version }}

- name: Remove Guava 14.0.1 from Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  file: path={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/guava-14.0.1.jar state=absent

- name: Download Guava 19.0 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ guava_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/guava-19.0.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Remove Spark SQL Kafka 2.12 2.4.4 from Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  file: path={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-2.4.4.jar state=absent

- name: Download Spark SQL Kafka 2.11 2.4.8 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ spark_sql_kafka_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.11-2.4.8.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Remove xbean 4.8 from Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  file: path={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar state=absent

- name: Download xbean 4.10 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ xbean_asm6_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/xbean-asm6-shaded-4.10.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Remove jets3t 0.9.3 from Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  file: path={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/jets3t-0.9.3.jar state=absent

- name: Download jets3t 0.9.4 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ jets3t_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/jets3t-0.9.4.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Download hadoop-aws 2.7.3 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ hadoop_aws_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Download hadoop-azure 2.7.3 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ hadoop_azure_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/hadoop-azure-2.7.3.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}


- name: Download azure_storage 3.0.0 and copy to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ azure_storage }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/azure-storage-3.0.0.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}


- name: Remove java-xmlbuilder 1.0 from Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  file: path={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar state=absent

- name: Download java-xmlbuilder 1.1 in to Spark jars folder
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url={{ java_xmlbuilder_url }} dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/jars/java-xmlbuilder-1.1.jar timeout=1000 force=no owner={{ analytics_user }} group={{ analytics_group }}

- name: Update spark default conf
  become: yes
  become_user: "{{ analytics_user }}"
  template: src=spark-defaults.j2 dest={{ spark.home }}/conf/spark-defaults.conf mode=755 owner={{ analytics_user }} group={{ analytics_group }}

- name: Update spark env
  become: yes
  become_user: "{{ analytics_user }}"
  template: src=spark-env.j2 dest={{ spark.home }}/conf/spark-env.sh mode=755 owner={{ analytics_user }} group={{ analytics_group }}

- name: Copy Jets3t properties file
  become: yes
  become_user: "{{ analytics_user }}"
  template: src=jets3t.j2 dest={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7/conf/jets3t.properties mode=755 owner={{ analytics_user }} group={{ analytics_group }}

## Provision DS ##
- name: Install packages
  become: yes
  apt:
    name: ["gcc", "autoconf", "curl", "g++", "gnupg", "automake", "bison", "libc6-dev", "libffi-dev", "libgdbm-dev", "libncurses5-dev", "libsqlite3-dev", "pkg-config", "sqlite3", "zlib1g-dev", "libtool", "libyaml-dev", "make", "libgmp-dev", "libreadline-dev", "libssl-dev", "gpgv2", "gem", "ruby", "gnupg2", "virtualenv"]
    state: installed
    update_cache: true
    cache_valid_time: 3600

- name: Import rvm keys
  become: yes
  become_user: "{{ analytics_user }}"
  shell: gpg --keyserver hkp://keyserver.ubuntu.com --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB

- name: Install rvm
  become: yes
  become_user: "{{ analytics_user }}"
  shell: curl -sSL https://get.rvm.io | bash -s stable

- name: source ruby script
  become: yes
  become_user: "{{ analytics_user }}"
  lineinfile:
    path: "{{ analytics.base_path }}/.bashrc"
    line: source /home/analytics/.rvm/scripts/rvm
    create: yes

- name: Change ownership
  file:
    path: "{{ analytics.base_path }}"
    owner: "{{ analytics_user }}"
    group: "{{ analytics_user }}"
    recurse: yes
  become: yes

- name: Install latest ruby
  become: yes
  become_user: "{{ analytics_user }}"
  shell: "export PATH=$PATH:/home/analytics/.rvm/bin && rvm install ruby-2.2"

- name: Add ruby repository
  become: yes
  apt_repository:
    repo: ppa:brightbox/ruby-ng

- name: Install latest ruby-dev
  become: yes
  apt:
    name: "ruby2.2-dev"
    state: installed
    update_cache: true
    cache_valid_time: 3600

- name: Install ruby-kafka
  become: yes
  become_user: "{{ analytics_user }}"
  shell: "bash -ilc 'export PATH=$PATH:/home/analytics/.rvm/bin && rvm --default use ruby-2.2 && gem install ruby-kafka'"

- name: Download Kafka-2.11
  become: yes
  become_user: "{{ analytics_user }}"
  get_url: url=http://downloads.mesosphere.com/kafka/assets/kafka_2.11-0.10.1.0.tgz dest={{ analytics.home }}/kafka_2.11-0.10.1.0.tgz force=no owner={{ analytics_user }} group={{ analytics_group }}
  tags:
    - kafka-provision

- name: Unarchive Kafka  
  become: yes
  become_user: "{{ analytics_user }}"
  unarchive: src={{ analytics.soft_path }}/kafka_2.11-0.10.1.0.tgz dest={{ analytics.home }}/ copy=no owner={{ analytics_user }} group={{ analytics_group }} creates={{ analytics.home }}/kafka_2.11-0.10.1.0
  tags:
    - kafka-provision

- name: Add python ppa
  become: yes
  apt_repository:
    repo: ppa:deadsnakes/ppa

- name: Install python 3.6
  become: yes
  apt:
    name: python3.6
    state: present
    update_cache: true
    cache_valid_time: 3600
