#!/usr/bin/env bash
set -e
# Configurations
druidCoordinatorIP="{{ druid.coordinator_host }}"
dataSourceName="{{ druid.data_source }}"
today=`date +%Y-%m-%d`
interval="2019-01-01_$today"
now=`date +%Y-%m-%d-%s`
home=`echo $HOME`
ingestionSpecFilePath="{{ druid.ingestion_spec_path }}"
jobJarPath="{{ content_snapshot_jar_path }}/{{ content_snapshot_jar_name }}"
jobConfPath="{{ job_config.es_cloud_uploader_path }}"
libs_path="{{ content_snapshot_path }}/jars/etl-jobs-1.0"
export SPARK_HOME="{{ spark_home }}"

# get list of segments from content model snapshot datasource
bkpIFS="$IFS"
segmentIds=$(curl -X 'GET' -H 'Content-Type:application/json' http://${druidCoordinatorIP}:8081/druid/coordinator/v1/metadata/datasources/${dataSourceName}/segments)
IFS=',]['
read -r -a array <<< ${segmentIds}
IFS="$bkpIFS"

echo "STARTED EXECUTING USER DRUID CONTENT INDEXER..."

# start the spark script to fetch Elasticsearch data and write it to a file and upload to cloud
#nohup {{ spark_home }}/bin/spark-submit \
#--conf spark.driver.extraJavaOptions="-Dconfig.file=${jobConfPath}" \
#--class org.ekstep.analytics.jobs.ESCloudUploader \
#${jobJarPath} >> "{{ content_snapshot_path }}/logs/$today-task-execution.log" 2>&1

nohup $SPARK_HOME/bin/spark-submit --conf spark.driver.extraJavaOptions="-Dconfig.file=${jobConfPath}"  --jars $(echo ${libs_path}/lib/*.jar | tr ' ' ',')  --class org.sunbird.analytics.jobs.ESCloudUploader ${jobJarPath} >> "{{ content_snapshot_path }}/logs/$today-task-execution.log" 2>&1

printf "\n>>> submit ingestion task to Druid!\n"

# submit task to start batch ingestion
curl -X 'POST' -H 'Content-Type:application/json' -d @${ingestionSpecFilePath} http://${druidCoordinatorIP}:8090/druid/indexer/v1/task


for index in "${!array[@]}"
do
    val=( $(eval echo ${array[index]}) )
    printf "\n>>> Disabling segment id: $val \n"
    # disable older segments
    curl -X 'DELETE' -H 'Content-Type:application/json' http://${druidCoordinatorIP}:8081/druid/coordinator/v1/datasources/${dataSourceName}/segments/$val
done

printf "\n>>> Deleting segments from interval $interval \n"

# delete older segments
curl -X 'DELETE' -H 'Content-Type:application/json' http://${druidCoordinatorIP}:8081/druid/coordinator/v1/datasources/${dataSourceName}/intervals/${interval}

printf "\n>>> success!\n"


