#
# Licensed to Metamarkets Group Inc. (Metamarkets) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. Metamarkets licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Extensions specified in the load list will be loaded by Druid
# We are using local fs for deep storage - not recommended for production - use S3, HDFS, or NFS instead
# We are using local derby for the metadata store - not recommended for production - use MySQL or Postgres instead

# If you specify `druid.extensions.loadList=[]`, Druid won't load any extension from file system.
# If you don't specify `druid.extensions.loadList`, Druid will load all the extensions under root extension directory.
# More info: http://druid.io/docs/latest/operations/including-extensions.html
druid.extensions.loadList=[{{ druid_extensions_list }}]
druid.extensions.directory={{ druid_path }}extensions

# Logging
# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties=true

# Zookeeper
druid.zk.service.host={{ druid_zookeeper_host }}
druid.zk.paths.base=/druid

# Metadata storage
# For PostgreSQL:
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://{{ druid_configs[cluster].druid_postgres_host }}:{{ druid_configs[cluster].druid_postgres_port }}/{{ druid_configs[cluster].druid_postgres_db }}
druid.metadata.storage.connector.user={{ druid_configs[cluster].druid_postgres_user }}
druid.metadata.storage.connector.password={{ druid_configs[cluster].druid_postgres_pass }}

# Deep storage
#Using azure
druid.storage.type={{ druid_storge_type }}
druid.azure.account={{ druid_configs[cluster].azure_account_name }}
druid.azure.key={{ druid_configs[cluster].azure_storage_secret }}
druid.azure.container={{ druid_configs[cluster].azure_container }}

# Indexing service logs
# For local disk (only viable in a cluster if this is a network mount):
druid.indexer.logs.type={{ druid_indexing_logs_type }}
#druid.indexer.logs.directory={{ druid_indexing_log_dir }}
druid.indexer.logs.container={{ druid_configs[cluster].druid_log_azure_container }}
druid.indexer.logs.prefix= {{ druid_configs[cluster].druid_log_azure_folder }}

# Service discovery
druid.selectors.indexing.serviceName={{ druid_overlord_service }}
druid.selectors.coordinator.serviceName={{ druid_coordinator_service }}

# Monitoring
druid.monitoring.monitors=[{{ druid_common_monitors }}]
druid.emitter=composing
druid.emitter.composing.emitters=[ {{ druid_common_emitters }}]
druid.emitter.logging.logLevel=info

{% if druid_monitoring %}
druid.emitter.graphite.port={{ druid_graphite_port | d(9109) }}
druid.emitter.graphite.hostname={{ druid_graphite_host | d("localhost") }}
druid.emitter.graphite.protocol=plaintext
druid.emitter.graphite.eventConverter={"type":"whiteList", "namespacePrefix": "{{ druid_graphite_prefix }}", "ignoreHostname":false, "ignoreServiceName":false ,"mapPath":"{{ druid_whitelist_filepath }}"}
{% endif %}


# Storage type of double columns
# ommiting this will lead to index double as float at the storage layer

druid.indexing.doubleStorage=double

#Writing query logs into file 
druid.request.logging.type={{ druid_request_logging_type }}
druid.request.logging.dir={{ druid_log_dir }}

druid.javascript.enabled=true
druid.sql.enable={{ enable_druid_sql | lower }}
