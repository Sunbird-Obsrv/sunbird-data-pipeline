#
# Licensed to Metamarkets Group Inc. (Metamarkets) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. Metamarkets licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Extensions specified in the load list will be loaded by Druid
# We are using local fs for deep storage - not recommended for production - use S3, HDFS, or NFS instead
# We are using local derby for the metadata store - not recommended for production - use MySQL or Postgres instead

# If you specify `druid.extensions.loadList=[]`, Druid won't load any extension from file system.
# If you don't specify `druid.extensions.loadList`, Druid will load all the extensions under root extension directory.
# More info: http://druid.io/docs/latest/operations/including-extensions.html
druid.extensions.loadList=[{{ druid_extensions_list }}]
druid.extensions.directory={{ druid_path }}extensions

# Logging
# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties=true

# Zookeeper
druid.zk.service.host={{ druid_zookeeper_host }}
druid.zk.paths.base=/druid

# Metadata storage
# For PostgreSQL:
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://{{ druid_configs[cluster].druid_postgres_host }}:{{ druid_configs[cluster].druid_postgres_port }}/{{ druid_configs[cluster].druid_postgres_db }}
druid.metadata.storage.connector.user={{ druid_configs[cluster].druid_postgres_user }}
druid.metadata.storage.connector.password={{ druid_configs[cluster].druid_postgres_pass }}

# Deep storage
#Using azure
druid.storage.type={{ druid_storage_type }}
{% if druid_storage_type == "azure" %}
druid.azure.account={{ druid_configs[cluster].azure_account_name }}
druid.azure.key={{ druid_configs[cluster].azure_storage_secret }}
druid.azure.container={{ druid_configs[cluster].azure_container }}
{% elif druid_storage_type == "s3" %}
druid.storage.bucket={{ druid_configs[cluster].s3_bucket }}
druid.storage.baseKey={{ druid_configs[cluster].s3_segment_dir }}
druid.s3.accessKey={{ druid_configs[cluster].s3_access_key }}
druid.s3.secretKey={{ druid_configs[cluster].s3_secret_key }}
# set protocol and endpoint together
druid.s3.endpoint.url={{ druid_configs[cluster].s3_endpoint }}
# to enable path like access
# if true,  url=<protocol>://<host>/<bucket>
# if false, url=<protocol>://<bucket>.<host>
druid.s3.enablePathStyleAccess={{ druid_configs[cluster].s3_path_like_access }}

# enable v4 signing of requests
druid.s3.endpoint.signingRegion={{ druid_configs[cluster].s3_v4_sign_region }}

# uncomment to enable access of bucket from any region
# druid.s3.forceGlobalBucketAccessEnabled=true
# uncomment to enable server side encryption for s3
# druid.storage.sse.type=s3
# uncomment to disable chunk encoding
# druid.s3.disableChunkedEncoding=true
{% endif %}

# Indexing service logs
# For local disk (only viable in a cluster if this is a network mount):
druid.indexer.logs.type={{ druid_indexing_logs_type }}
{% if druid_indexing_logs_type == "azure" %}
#druid.indexer.logs.directory={{ druid_indexing_log_dir }}
druid.indexer.logs.container={{ druid_configs[cluster].druid_log_azure_container }}
druid.indexer.logs.prefix= {{ druid_configs[cluster].druid_log_azure_folder }}
{% elif druid_indexing_logs_type == "s3" %}
druid.indexer.logs.s3Bucket={{ druid_configs[cluster].s3_logging_bucket }}
# path to logs within the bucker
druid.indexer.logs.s3Prefix={{ druid_configs[cluster].s3_indexer_logs_dir }}
{% endif %}

# Service discovery
druid.selectors.indexing.serviceName={{ druid_overlord_service }}
druid.selectors.coordinator.serviceName={{ druid_coordinator_service }}

# Monitoring
druid.monitoring.monitors=[{{ druid_common_monitors }}]
druid.emitter=composing
druid.emitter.composing.emitters=[ {{ druid_common_emitters }}]
druid.emitter.logging.logLevel=info

{% if druid_monitoring %}
druid.emitter.graphite.port={{ druid_graphite_port | d(9109) }}
druid.emitter.graphite.hostname={{ druid_graphite_host | d("localhost") }}
druid.emitter.graphite.protocol=plaintext
druid.emitter.graphite.eventConverter={"type":"whiteList", "namespacePrefix": "{{ druid_graphite_prefix }}", "ignoreHostname":false, "ignoreServiceName":false ,"mapPath":"{{ druid_whitelist_filepath }}"}
{% endif %}


# Storage type of double columns
# ommiting this will lead to index double as float at the storage layer

druid.indexing.doubleStorage=double

#Writing query logs into file 
druid.request.logging.type={{ druid_request_logging_type }}
druid.request.logging.dir={{ druid_log_dir }}

druid.javascript.enabled=true
druid.sql.enable={{ enable_druid_sql | lower }}
