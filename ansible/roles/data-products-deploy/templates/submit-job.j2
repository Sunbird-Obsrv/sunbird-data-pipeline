#!/usr/bin/env bash

export SPARK_HOME={{ analytics.home }}/spark-2.0.1-bin-hadoop2.7
export MODELS_HOME={{ analytics.home }}/models
export DP_LOGS={{ analytics.home }}/logs/data-products
export KAFKA_HOME={{ analytics.soft_path }}/kafka_2.11-0.10.1.0

## job broker-list and kafka-topic
job_brokerList={{ brokerlist }}
job_topic={{ analytics_job_queue_topic }}

## Job to run daily
cd {{ analytics.home }}/scripts
source model-config.sh
today=$(date "+%Y-%m-%d")

if [ -z "$job_config" ]; then job_config=$(config $1); fi

echo "Submitted $1 with config $job_config" >> "$DP_LOGS/$today-job-execution.log"
echo '{ "model" :' \"$1\" ',' ' "config": ' "$job_config" '}' >> "$DP_LOGS/$today-job-execution-debug.log"
echo '{ "model" :' \"$1\" ',' ' "config": ' "$job_config" '}' > /tmp/job-request.json
cat /tmp/job-request.json | $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list $job_brokerList --topic $job_topic >> "$DP_LOGS/$today-job-execution.log" 2>&1
