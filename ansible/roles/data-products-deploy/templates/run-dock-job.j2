#!/usr/bin/env bash

export SPARK_HOME={{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7
export MODELS_HOME={{ analytics.home }}/models-{{ model_version }}
export DP_LOGS={{ analytics.home }}/logs/data-products
## Job to run daily
cd {{ analytics.home }}/scripts
source model-dock-config.sh
today=$(date "+%Y-%m-%d")

libs_path="{{ analytics.home }}/models-{{ model_version }}/data-products-1.0"
file_path="dock-{{ env }}.conf"

get_report_job_model_name(){
	case "$1" in
		"funnel-report") echo 'org.sunbird.analytics.sourcing.FunnelReport'
		;;
		"sourcing-summary-report") echo 'org.sunbird.analytics.sourcing.SourcingSummaryReport'
		;;
		"sourcing-metrics") echo 'org.sunbird.analytics.sourcing.SourcingMetrics'
		;;
		"content-details") echo 'org.sunbird.analytics.sourcing.ContentDetailsReport'
		;;
		*) echo $1
		;;
	esac		
}

if [ ! -z "$1" ]; then job_id=$(get_report_job_model_name $1); fi

if [ ! -z "$1" ]; then job_config=$(config $1); else job_config="$2"; fi

if [ ! -z "$2" ]; then batchIds=";$2"; else batchIds=""; fi

echo "Starting the job - $1" >> "$DP_LOGS/$today-job-execution.log"

echo "Job modelName - $job_id" >> "$DP_LOGS/$today-job-execution.log"

nohup $SPARK_HOME/bin/spark-submit  --conf spark.driver.extraJavaOptions="-Dconfig.file=$MODELS_HOME/$file_path" --conf spark.executor.extraJavaOptions="-Dconfig.file=$MODELS_HOME/$file_path" --master local[*] --jars $(echo ${libs_path}/lib/*.jar | tr ' ' ','),$MODELS_HOME/analytics-framework-2.0.jar,$MODELS_HOME/scruid_2.11-2.4.0.jar,$MODELS_HOME/batch-models-2.0.jar --class org.ekstep.analytics.job.JobExecutor $MODELS_HOME/batch-models-2.0.jar --model "$job_id" --config "$job_config$batchIds" \ >> "$DP_LOGS/$today-job-execution.log" 2>&1

echo "Job execution completed - $1" >> "$DP_LOGS/$today-job-execution.log"