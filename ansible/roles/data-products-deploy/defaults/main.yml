analytics_user: analytics
analytics_group: analytics
spark_output_temp_dir: /mount/data/analytics/tmp/
telemetry_converter_ver: 0.0.8

bucket: "dev-data-store"
kafka_broker_host: "{{groups['processing-cluster-kafka'][0]}}:9092"
brokerlist: "{{groups['processing-cluster-kafka']|join(':9092,')}}:9092"
zookeeper: "{{groups['processing-cluster-zookeepers']|join(':2181,')}}:2181"
dp_username: dp-monitor
analytics_job_queue_topic: "{{ env }}.analytics.job_queue"
datasetRawBucket: ekstep-data-sets-{{ env }}
dataExhaustBucket: ekstep-public-{{ env }}
dataExhaustPrefix: data-exhaust/
consumptionRawPrefix: datasets/D001/4208ab995984d222b59299e5103d350a842d8d41/
topic: "{{ env }}.telemetry.derived"
learning_topic: "{{ env }}.learning.graph.events"
analytics_metrics_topic: "{{ env }}.analytics_metrics"
sink_topic: "{{ env }}.telemetry.sink"
metrics_topic: "{{ env }}.telemetry.metrics"
job_manager_tmp_dir: "transient-data"
channel: dev-test
druid_broker_host: "{{groups['raw-broker'][0]}}"
druid_rollup_broker_host: "{{groups['raw-broker'][0]}}"
hierarchySearchServiceUrl: "{{ proto }}://{{ domain_name }}/action/content"
hierarchySearchServicEndpoint: /v3/hierarchy/

user_table_keyspace: "sunbird"
course_keyspace: "sunbird_courses"
hierarchy_store_keyspace: "{{ env }}_hierarchy_store"
job_request_table: "{{ env }}_job_request"


analytics_job_list: '"wfs", "ds", "dpu", "content-rating-updater", "audit-metrics-report", "monitor-job-summ"'
analytics_jobs_count: 8

cassandra_keyspace_prefix: '{{ env }}_'
cassandra_hierarchy_store_keyspace: "{{ env_name}}_hierarchy_store"
dp_dispatch_bucket_name: ekstep-dev-data-store
data_exhaust_blob_name: sunbird1p7
data_exhaust_s3_url: https://s3-ap-south-1.amazonaws.com
spark_version: 2.4.4

heap_memory: "-Xmx5120m"

# Adding dock broker ip for data-products
dock:
  brokerList: ""
  env: ""

business_metrics:
  s3_path: "s3://ekstep-{{ env }}-data-store/business_metrics/"
  env: "{{ env }}"

spark:
  home: "{{ analytics.home }}/spark-{{ spark_version }}-bin-hadoop2.7"
  public_dns: 54.255.154.146
  master:
    url: spark://172.31.11.117:7077
    host: 172.31.11.117
  worker:
    instances: 1
    cores: 2
    memory: 4g
  driver:
    memory: 3g
  executor:
    memory: 4g
  driver_memory: 7g
  memory_fraction: 0.3
  storage_fraction: 0.5  
  executor_memory: 2g
  heap_conf_str: '"-XX:+UseG1GC -XX:MaxGCPauseMillis=100 -Xms250m {{ heap_memory }} -XX:+UseStringDeduplication"'

submit_jobs:
  submit-all-jobs:
    hour: 02
    minute: 35

start_jobmanager:
  job-manager:
    hour: 02
    minute: 30
have_weekly_jobs: false

video_stream_job_schedule: 10

run_wfs_job:
  wfs:
    hour: 00
    minute: 30
run_monitor_job:    
  monitor-job-summ:
    hour: 03
    minute: 00

run_course_metrics_job:
  course-dashboard-metrics:
    hour: 19
    minute: 45

run_assessment_metrics_job:
  assessment-dashboard-metrics:
    hour: 20
    minute: 15

run_admin_user_reports_job:
  admin-user-reports-3AMIST:
    hour: 21
    minute: 30
  admin-user-reports-2PMIST:
    hour: 8
    minute: 30  

run_admin_geo_reports_job:
  admin-geo-reports-4AMIST:
    hour: 22
    minute: 30
  admin-geo-reports-3PMIST:
    hour: 9
    minute: 30     

run_assessment_aggregator_report_job:
  assessment-aggregator-report:
    hour: 18
    minute: 35

update_user_redis_cache:
  populate-user-cache:
    hour: 3
    minute: 00

index_content_model_druid:
  index_content:
    hour: 1
    minute: 00

run_district_weekly_job:
  district-weekly:
    hour: 03
    minute: 30
    weekday: 1

run_etb_metrics_weekly_job:
  etb-metrics-weekly:
    hour: 23
    minute: 30
    weekday: 1

# These are the dummy times till sept30 for exhaust reports
#To-Do: Update time after 3.2.7 deployment

run_progress_exhaust:
  progress-exhaust:
    hour: 08
    minute: 00

run_response_exhaust:
  response-exhaust:
    hour: 09
    minute: 00

run_userinfo_exhaust:
  userinfo-exhaust:
    hour: 10
    minute: 00

run_district_monthly_job:
  district-monthly:
    hour: 04
    minute: 30
    day: 1
    month: 1-12


run_collection_summary:
  collection-summary:
    hour: 09
    minute: 30


service:
  search:
    url: http://{{private_ingressgateway_ip}}/search
    path: /v3/search

es_search_index: "compositesearch"
analytics:
  home: /mount/data/analytics
  soft_path: /mount/data/analytics
  paths: ['/home/analytics/sbin', '/mount/data/analytics', '/mount/data/analytics/logs', '/mount/data/analytics/logs/services', '/mount/data/analytics/logs/data-products', '/mount/data/analytics/logs/api-service', '/mount/data/analytics/api', '/mount/data/analytics/tmp', '/mount/data/analytics/scripts/monitor-data', '/mount/data/analytics/scripts', '/mount/data/analytics/models' ]
  scripts: ['model-config', 'replay-job', 'replay-updater', 'replay-utils', 'run-job', 'monitor-dp', 'generate-metrics', 'submit-job', 'start-jobmanager', 'run-dock-job','submit-script']


producer_env: "dev.sunbird"
analytics_job_manager_artifact: "job-manager-2.0.jar"
analytics_core_artifact: "analytics-framework-2.0.jar"
scruid_artifact: "scruid_2.11-2.4.0.jar"
analytics_batch_module_artifact: "batch-models-2.0.jar"
analytics_ed_dataporducts_artifact: "data-products-1.0-distribution.tar.gz"
model_version: "2.0" 

submit_jobs_auth_token: "{{ sunbird_api_auth_token }}"
report_list_jobs_url: "{{ druid_report_url }}"

reports_container: "reports"

spark_redis_url: https://repo1.maven.org/maven2/com/redislabs/spark-redis_2.11/2.5.0/spark-redis_2.11-2.5.0.jar
jedis_url: https://repo1.maven.org/maven2/redis/clients/jedis/3.2.0/jedis-3.2.0.jar

spark_redis_version: "spark-redis_2.11-2.5.0.jar"
jedis_version: "jedis-3.2.0.jar"
