input {
        kafka {
            bootstrap_servers => "{{bootstrap_server}}"
            topics => ["{{kafka_topic_prefix}}.{{learning_topics}}"]
            group_id => "{{kafka_topic_prefix}}.learning.events"
            auto_offset_reset => "latest"
            codec => "json"
            consumer_threads => 1
            fetch_max_bytes => "1048576"
            type => "events"
            add_field => ["learning","true"]
            decorate_events => false
        }
}

filter {
	if [learning] == "true" and ![metadata][learning_index]{
		if [context] and [context][date_range] and [context][date_range][to]{
			if [context][granularity] == "CUMULATIVE"{
				ruby {
					code => "event['[metadata][learning_index]'] = 'cumulative'"
				}
			}
			else{
				ruby {
					code => "event['[metadata][learning_index]'] = DateTime.strptime(event['[context][date_range][to]'].to_s,'%Q').to_time.strftime('%Y.%m')"
				}
			}

		}
	}
}

output {
	if [type] == "events" and [learning] == "true" and [ready_to_index] != "true" and [retry] != "true" and [metadata][public] != "true" {
        kafka {
            bootstrap_servers => "{{kafka_brokers}}"
            topic_id => "{{kafka_topic_prefix}}{{kafka_sink_topic}}"
            compression_type => "none"
            acks => "0"
            value_serializer => "org.apache.kafka.common.serialization.StringSerializer"
            metadata_fetch_timeout_ms => 10000
            key_serializer => "org.apache.kafka.common.serialization.StringSerializer"
            retries => 3
            retry_backoff_ms => 100
            metadata_max_age_ms => 600000
            batch_size => 200
            send_buffer_bytes => 102400
            client_id => "logstash"
        }
	}
}
