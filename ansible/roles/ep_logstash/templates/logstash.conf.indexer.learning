input {
    kafka {
        bootstrap_servers => "{{bootstrap_server}}"
        topics => ["{{kafka_topic_prefix}}.{{learning_topics}}"]
        group_id => "{{kafka_topic_prefix}}.learning.events"
        auto_offset_reset=>"latest"
        codec => "json"
        consumer_threads=>1
        fetch_max_bytes=>"1048576"
        type => "events"
        add_field =>["learning","true"]
        decorate_events => false
    }
}

filter {
  if [learning] == "true" and ![metadata][learning_index]{
    if [context] and [context][date_range] and [context][date_range][to]{
      if [context][granularity] == "CUMULATIVE"{
        ruby {
          code => "event['[metadata][learning_index]'] = 'cumulative'"
        }
      }
      else{
        ruby {
          code => "event['[metadata][learning_index]'] = DateTime.strptime(event['[context][date_range][to]'].to_s,'%Q').to_time.strftime('%Y.%m')"
        }
      }

    }
  }
}

output {
	if [type] == "events" and [learning] == "true" and [ready_to_index] != "true" and [retry] != "true" and [metadata][public] != "true" {
      kafka {
        bootstrap_servers => "{{kafka_brokers}}"
        topic_id => "{{kafka_topic_prefix}}{{kafka_sink_topic}}"
        compression_type => "none"
        acks => "0"
        value_serializer => "org.apache.kafka.common.serialization.StringSerializer"
        metadata_fetch_timeout_ms => 10000
        key_serializer => "org.apache.kafka.common.serialization.StringSerializer"
        retries => 3
        retry_backoff_ms => 100
        metadata_max_age_ms => 600000
        batch_size => 200
        send_buffer_bytes => 102400
        client_id => "logstash"
    }
  }
}
